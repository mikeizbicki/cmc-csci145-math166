\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{defn}{Definition}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\begin{center}
{
\Huge
Notes: Computational Linear Algebra
}

\vspace{0.15in}
Due: Sunday, 30 Aug 2020 by midnight
\end{center}

\begin{center}
\includegraphics[height=1.8in]{comic}
\includegraphics[height=1.8in]{matrix_transform}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pre-lecture work}

\begin{problem}
    (optional) Watch the 3blue1brown's videos on linear algebra:
    \begin{quote}
        \url{https://www.3blue1brown.com/essence-of-linear-algebra-page}
    \end{quote}
    The eigenvalue video is the most important:
    \begin{quote}
        \url{https://www.youtube.com/watch?v=PFDu9oVAE-g}
    \end{quote}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Review: Big-O/$\Theta$/$\Omega$ Notation}

\begin{defn}
    Let $f,g$ be (possibly multivariate) functions from $(\R^+)^d\to\R^+$,
    where $d>0$ is an integer that specifies the number of input variables.
    Then,
    \begin{enumerate}
        \item If $\displaystyle\lim_{\x\to\infty} \frac{f(\x)}{g(\x)} < \infty$, then we say $f = O(g)$.
        \item If $\displaystyle\lim_{\x\to\infty} \frac{f(\x)}{g(\x)} > 0$, then we say $f = \Omega(g)$.
        %\item If $\lim_{x\to\infty} \frac{f(x)}{g(x)} = 0$, then we say $f = O(g)$.
        \item We say that $f = \Theta(g)$ if both $f=O(g)$ and $f=\Omega(g)$.
    \end{enumerate}
    Intuitively, you should think of $O$ as $\le$, $\Omega$ as $\ge$, and $\Theta$ as $=$.
\end{defn}



\newpage
\begin{problem}
    Complete each equation below by adding the symbol $O$ if $f=O(g)$, $\Omega$ if $f=\Omega(g)$, or $\Theta$ if $f=\Theta(g)$.  
    The first row is completed for you as an example.

{\renewcommand{\arraystretch}{4.4}
\begin{tabular}{c c c c c c}
    & f(n) &~\hspace{0.5in}~$ $~\hspace{0.5in}~& g(n) &\\
    \hline
    & $1$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $O(n)$ &  &\\
    \arrayrulecolor{gray}\hline
    & $3 n\log n$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $n^2$ &  &\\
    \arrayrulecolor{gray}\hline
    & $1$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $1/n$ &  &\\
    \arrayrulecolor{gray}\hline
    & $\log_2 n$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $\log_3 n$ &  &\\
    \arrayrulecolor{gray}\hline
    & $\log n$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $\frac {1} {\log n}$ &  &\\
    \arrayrulecolor{gray}\hline
    & $5\cdot10^{30}$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $\log n$ &  &\\
    \arrayrulecolor{gray}\hline
    & $\log n$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $\log (n^2)$ &  &\\
    \arrayrulecolor{gray}\hline
    & $2^n$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $3^n$ &  &\\
    \arrayrulecolor{gray}\hline
    & $\frac 1 n$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $\sqrt{\frac 1 n}$ &  &\\
    \arrayrulecolor{gray}\hline
    & $\log n$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $(\log n)^2$ &  &\\
    \arrayrulecolor{gray}\hline

    %& $O(1)$ & or & $O(n)$ & or & equal\\
    %& $O(n\log n)$ & or & $O(n^2)$ & or & equal\\
    %& $\Theta(1)$ & or & $\Theta(1/n)$ & or & equal\\
    %& $\Omega(\log_2 n)$ & or & $\Omega(\log_3 n)$ & or & equal\\
    %& $O(n^{42})$ & or & $O(42^n)$ & or & equal\\
    %& $\Theta(5\cdot10^{30})$ & or & $\Theta(\log n)$ & or & equal\\
    %& $\Omega(\log n)$ & or & $\Omega(\log (n^2))$ & or & equal\\
    %& $O(2^n)$ & or & $O(3^n)$ & or & equal\\
    %& $\Theta(n!)$ & or & $\Theta(n^2)$ & or & equal\\
    %& $\Omega(\log n)$ & or & $\Omega((\log n)^2)$ & or & equal\\
\end{tabular}
}
\end{problem}

\newpage
\begin{problem}
    Complete each equation below by adding the symbol $O$ if $f=O(g)$, $\Omega$ if $f=\Omega(g)$, or $\Theta$ if $f=\Theta(g)$.  
    If $f$ cannot be related to $g$ using asymptotic notation, draw a slash through the equals sign.
    The first row is completed for you as an example.

{\renewcommand{\arraystretch}{4.4}
\begin{tabular}{c c c c c c}
    & f(a,b,c) &~\hspace{0.5in}~$ $~\hspace{0.5in}~& g(a,b,c) &\\
    \hline
    & $ab$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $\Omega(b)$ &  &\\
    \arrayrulecolor{gray}\hline
    & $a^2b$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $ab^2$ &  &\\
    \arrayrulecolor{gray}\hline
    & $a\log b$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $a^b$ &  &\\
    \arrayrulecolor{gray}\hline
    & $a^2bc^3$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $a^2b^2c^3$ &  &\\
    \arrayrulecolor{gray}\hline
    & $\frac ab$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $\frac a {b^2}$ &  &\\
    \arrayrulecolor{gray}\hline
    & $\frac ab$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $(\frac a b)^2$ &  &\\
    \arrayrulecolor{gray}\hline
    & $a^b$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $b^a$ &  &\\
    \arrayrulecolor{gray}\hline
    & $a^b$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $(\log a)^c$ &  &\\
    \arrayrulecolor{gray}\hline
    & $a^b$ & ~\hspace{0.5in}~$=$~\hspace{0.5in}~  & $(1+c)^a$ &  &\\
    \arrayrulecolor{gray}\hline

    %& $O(1)$ & or & $O(n)$ & or & equal\\
    %& $O(n\log n)$ & or & $O(n^2)$ & or & equal\\
    %& $\Theta(1)$ & or & $\Theta(1/n)$ & or & equal\\
    %& $\Omega(\log_2 n)$ & or & $\Omega(\log_3 n)$ & or & equal\\
    %& $O(n^{42})$ & or & $O(42^n)$ & or & equal\\
    %& $\Theta(5\cdot10^{30})$ & or & $\Theta(\log n)$ & or & equal\\
    %& $\Omega(\log n)$ & or & $\Omega(\log (n^2))$ & or & equal\\
    %& $O(2^n)$ & or & $O(3^n)$ & or & equal\\
    %& $\Theta(n!)$ & or & $\Theta(n^2)$ & or & equal\\
    %& $\Omega(\log n)$ & or & $\Omega((\log n)^2)$ & or & equal\\
\end{tabular}
}
\end{problem}

\newpage
\begin{problem}
    Simplify the following expressions:

\begin{enumerate}
    \item $O\bigg(n^3 + 5n^2\log n + \log n \bigg)$
    \vspace{1.5in}
\item $O\bigg(ab^2 + 3a^2b + ab + 10b\bigg)$
    \vspace{1.5in}
\item $O\bigg(a + b + c + ab + bc + abc\bigg)$
    \vspace{1.5in}
\item $O\bigg(\frac 1 n + \frac 1 {n^2}\bigg)$
    \vspace{1.5in}
\item $O\bigg(\frac 1 n + \frac 1 {nm} + \frac 1 m \bigg)$
    \vspace{1.5in}
%\item $\Omega\bigg((3.45n + n)(\log n^2)\bigg)$
    %\vspace{1.5in}
%\item $\Theta\bigg(n(1 + \log n) + n^{3.2} + \log 2^n\bigg)$
    %\vspace{1.5in}
\end{enumerate}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{BLAS/LAPACK}

The \emph{Basic Linear Algebra Subprogram} (BLAS) and \emph{Linear Algebra Package} (LAPACK) are the primary low-level tools of computational linear algebra.
These high performance libraries are written in Assembly, C, and Fortran.
All programming languages (R, Python, Matlab, etc.) convert your code into basic BLAS/LAPACK function calls,
and rely on these libraries for good performance.

There are many different versions of BLAS/LAPACK depending on:
\begin{enumerate}
    \item The format of matrices (dense, sparse, diagonal, toplitz, etc.)
    \item The computer architecture (Intel vs ARM, 8/16/32/64/128 bit, parallel CPUs, GPUs, clusters, etc.)
\end{enumerate}
In this class, we will be using the PyTorch library,
and not directly interfacing with BLAS/LAPACK.
It is still important to understand BLAS/LAPACK to understand the performance of your algorithms.
PyTorch supports Dense and Sparse formats on both parallel CPU and CUDA architectures.

\subsection{Dense vs Sparse Matrices}

Read the intro of this wikipedia article:
\begin{quote}
    \url{https://en.wikipedia.org/wiki/Sparse_matrix}
\end{quote}
We will be using $\nnz(A)$ to denote the sparsity of $A:\R^{m\times n}$.
If $A$ is stored as a sparse matrix,
the memory usage is $\Theta(\nnz(A))$.
If $A$ is stored as a dense matrix,
the memory usage is $\Theta(mn)$.

\begin{problem}
    Prove or provide a counter example.
    For both sparse and dense matrices,
    the storage requirements are $O(mn)$.
    \vspace{2in}
\end{problem}

\begin{problem}
    A computer has 8GB of RAM.
    What is the largest dense vector of 64bit floating point numbers that can fit in memory?
    What about 32bit?
    \vspace{2in}
\end{problem}

\subsection{BLAS Level 1}

BLAS Level 1 routines are for vector-vector operations.
These include the dot product, vector norms, addition, and scalar multiplication.

These are fully implemented in PyTorch for both sparse and dense vectors.
For dense vectors of length $n$, that is $\Theta(n)$.
For sparse vectors, all of operations of a single vector $\x$ take time $\Theta(\nnz(\x))$
and all operations of two vectors $\x$ and $\y$ take time $\Theta(\nnz(\x)+\nnz(\y))$.

\begin{problem}
    What is the runtime of the following expressions:
    \begin{enumerate}
        \item $5 \cdot \x$
            \vspace{1in}
        \item $0 \cdot \x$
            \vspace{1in}
        \item $\trans\x \y$
            \vspace{1in}
        \item $\lone{\x+\y}$
            \vspace{1in}
    \end{enumerate}
\end{problem}

\begin{problem}
    What is the formula for the following vector norms:
    \begin{enumerate}
        \item $\lone{\x}=$
            \vspace{1in}
        \item $\ltwo{\x}=$
            \vspace{1in}
        \item $\linf{\x}=$
            \vspace{1in}
        \item $\lp{\x}=$
            \vspace{1in}
    \end{enumerate}
\end{problem}

\subsection{BLAS Level 2}

BLAS Level 2 routines are for vector-matrix operations.
The main example is the product $A\x$, 
where $A : \R^{m\times n}$ and $\x : \R^{n}$.
PyTorch fully supports these operations in dense matrices,
but has only partial support in sparse matrices.
The sparse matrix support is via BLAS Level 3,
so we will only discuss that.

\subsection{BLAS Level 3}

BLAS Level 3 routines are for matrix-matrix operations.
Let $A : \R^{m\times n}$ and $B : \R^{n \times o}$.
Note that when $o=1$, then $B$ is a vector, and so BLAS Level 3 operations can be used to implement BLAS Level 2 operations.

For dense matrices, the matrix multiplication $AB$ takes time $O(mno)$.
This is often referred to as a ``cubic'' runtime because when the multiplied matrices are square,
and thus $m=n=o$,
the runtime is $O(n^3)$.
There are good algorithms that take potentially much less time than this,
%and you should read the wikipedia entry:
with the best known taking time $O(n^{2.373})$.
%And yes, that many significant figures is actually necessary,
No BLAS implementation that I know of actually implements this algorithm,
and Strassen's algorithm with runtime $\Theta(n^{\log_2 7})\approx\Theta(n^{2.807})$ is the most commonly used algorithm.
Currently, we know that matrix multiplication takes at least time $\Omega(m^2\log m)$,
and it is an open problem what the optimal runtime for matrix multiplication is.
The following wikipedia link has more details:
\begin{quote}
\url{https://en.wikipedia.org/wiki/Matrix_multiplication#Computational_complexity}
\end{quote}

For sparse matrices, PyTorch has only limited support for BLAS Level 2/3.
In particular, PyTorch allows the first matrix to be either sparse or dense,
but the second matrix must be dense.
The runtime in the sparse case is $O(\nnz(A)no)$.
The output is always a dense tensor.

For both sparse and dense matrices,
the runtime of computing the transpose is $\Theta(1)$.
%The library does not actually compute the transpose,
%but just stores a flag that the matrix has been transposed.

\begin{problem}
    What is the runtime of the following expressions?
    (Assume that the dimensions match so that the expressions are well defined.)

    \begin{enumerate}
        \item
            $A + B$
            \vspace{1in}
        \item
            $AB$
            \vspace{1in}
        \item
            $\trans B \trans A$
            \vspace{1in}
        \item
            $A\x$
            \vspace{1in}
        \item
            $\trans \x \x$
            \vspace{1in}
        \item
            $\x\trans \x$
            \vspace{1in}
        \item
            $A\x + \x$
            \vspace{1in}
        \item
            $(A+I)\x$
            \vspace{1in}
    \end{enumerate}
\end{problem}

\newpage
\begin{problem}
    Many systems that implement sparse matrices offer only limited support in the same way that PyTorch does.
    The reason is that the sparsity of the matrix product $AB$ is impossible to predict from the sparsity of just $A$ and $B$.
    In this problem, you will prove this fact by example.
    Your task is to construct matrices $A$ and $B$ such that
    \begin{equation}
        \nnz(AB) = O(1)
        \qquad
        \text{and}
        \qquad
        %\nnz(\trans B \trans A) = O(mno)
        \nnz(\trans B \trans A) = \Omega(mo)
        .
    \end{equation}
    \vspace{3in}
\end{problem}

\begin{problem}
    (Optional)
    Strassen's algorithm is a famous divide and conquer algorithm for fast matrix multiplication.
    It is the most commonly implemented algorithm in BLAS libraries because it has both good constant factors and good asymptotic performance.
    Lookup this algorithm and understand why it works.
    This is a problem that every computer science undergrad should do at some point in their undergrad career.
    \vspace{4in}
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Matrix Chain Ordering Problem (MCOP)}

Matrix multiplication is famously not commutative, but it is associative.
That is, no matter where you place the parentheses,
you will get the same answer.
Computationally, however, some choices of parentheses can be much better than others.

\begin{problem}
    Let $A : \R^{m\times n}$, $B : \R^{n\times o}$, $C : \R^{o\times p}$.

    \begin{enumerate}
        \item What is the runtime of computing $(AB)C$?
            \vspace{2in}
        \item What is the runtime of computing $A(BC)$?
            \vspace{2in}
        \item Under what conditions would you choose the former parenthesization over the latter?
            \vspace{2in}
        \newpage
        \item
            Recall that in PyTorch, only the first matrix in a matrix multiplication can be sparse,
            and the second must be dense.
            It is still possible to compute the product $ABC$ if one of the matrices is dense and the other two are sparse.
            For example, if $A$ and $B$ are sparse, then the parenthesization $A(BC)$ can be computed in PyTorch.
            How should you rewrite the multiplication $ABC$ so that it can be computed when both $A$ and $C$ are sparse?

            \noindent
            Hint: You can swap the order of matrix multiplications by using the fact that for any two matrices $X$ and $Y$, $XY = \trans{(\trans Y \trans X)}$.
            \vspace{2in}
    \end{enumerate}
\end{problem}

The \emph{Matrix Chain Ordering Problem} (MCOP) is the problem of finding the optimal ordering of parentheses for a given sequence of $n$ matrices.
There is a classic dynamic programming solution to this problem that takes time $\Theta(n^3)$.
Many more complicated algorithms exist for solving MCOP,
the best of which currently takes time $\Theta(n \log n)$.
As far as I know, the best lower bound for MCOP is $\Omega(n)$,
and it is therefore an open problem whether MCOP can be solved in linear time.

\textbf{Note:}
The $\Theta$ and $\Omega$ notation above is correct.
It should be obvious why these values are consistent with each other and do not contradict the definitions of $\Theta$ and $\Omega$.
If it's not obvious to you, you should ask and get this clarified.

\begin{problem}
    (Optional)
    Give the dynamic programming solution for MCOP.
    This is a problem that every computer science undergrad should do at some point in their undergrad career.
    \vspace{4in}
\end{problem}

\newpage
\begin{problem}
    \label{prob:8}
    Parenthesization becomes critical when the matrices are vectors.
    What is the optimal way to parenthesize the expressions:
    \begin{enumerate}
        \item
            $\trans \x \x \trans \x \x$?
            \vspace{2in}
        \item
            $\x \trans \x \x \trans \x$?
            \vspace{2in}
    \end{enumerate}
\end{problem}

\begin{problem}
    Based on your solution to Problem \ref{prob:8} above,
    what is the asymptotically fastest way to compute the following expression
    \begin{equation}
        \prod_{i=1}^n (\x \trans \x)
    \end{equation}
    \vspace{2in}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\subsection{LAPACK}

LAPACK provides more complex matrix operations than BLAS,
and LAPACK functions are typically internally written in terms of BLAS operations.
PyTorch has full support for dense matrices,
and no support for sparse matrices.

Let $A : \R^{n \times n}$.
Then the matrix inverse $A^{-1}$ and matrix determinant $\determinant(A)$ can both be reduced to matrix multiplication problems.
The runtime is therefore the same as matrix multiplication, which we talk about as $O(n^3)$ even though $O(n^{2.373})$ algorithms exist.
The runtime of computing the top $k$ eigenvectors and eigenvalues is $O(n^2k)$.

\begin{problem}
    Let $\x : \R^n$
    and 
    $A : \R^{n\times n}$.
    What is the formula for the following matrix norms?
    (Note that the $\ltwo{\cdot}$ and $\lp{\cdot}$ norms are defined differently for matrices than vectors.)
    \begin{enumerate}
        \item $\ltwo{A}$
            \vspace{2in}
        \item $\lp{A}$
            \vspace{2in}
        \item $\lF{A}$
            \vspace{2in}
    \end{enumerate}
\end{problem}

\newpage
\begin{problem}
    Use the formulas above to calculate the runtime of computing the following matrix norms.
    \begin{enumerate}
        \item $\ltwo{A}$
            \vspace{2in}
        \item $\lF{A}$
            \vspace{2in}
    \end{enumerate}
    We do not have enough tools yet for calculating the runtime of generic $\lp{\cdot}$ norms,
    and that's why I didn't ask you to provide that runtime.
\end{problem}

\begin{problem}
    Let $A : \R^{m \times n}$, $\x : \R^n$, and $\lambda : \R$.
    What is the runtime for computing the following matrix expressions assuming $A$ and $\x$ are dense?
    \begin{enumerate}
        \item $(A \trans A)^{-1} A \x$
            \vspace{2in}
        \item $A (\trans A A)^{-1}$
            \vspace{2in}
        \item $\ltwo{A \trans A A \x + \lambda \x}^2$
            \vspace{2in}
        \item $\lF{(\lambda I + A) \x \trans \x}$

            Note that for the expression above to be well defined, $m$ must be equal to $n$.
            \vspace{2in}
    \end{enumerate}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{(Optional) Tensors}

Tensors are higher order versions of vectors and matrices,
and PyTorch is designed to handle tensors efficiently.
There are many open problems related to computing tensor operations efficiently,
but PyTorch implements its tensor operations by ``reducing'' them to BLAS/LAPACK operations.
When you use the \texttt{einsum} function,
PyTorch compiles your Einstein summation notation into the most efficient possible combination of BLAS/LAPACK operations.
That is why I personally always use \texttt{einsum} whenever possible,
and I recommend you do to.
\texttt{einsum} is not compatible with sparse tensors.


\end{document}


