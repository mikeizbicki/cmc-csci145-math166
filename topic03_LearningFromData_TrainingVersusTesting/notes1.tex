\documentclass[10pt]{exam}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{stmaryrd}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

\usepackage{listings}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{defn}{Definition}
\newtheorem{note}{Note}
\newtheorem{fact}{Fact}
\newtheorem{refr}{References}
\newtheorem{theorem}{Theorem}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\prob}{\mathbb P}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\Ein}{E_{\text{in}}}
\newcommand{\Eout}{E_{\text{out}}}
\newcommand{\Etest}{E_{\text{test}}}
\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\wstar}{{\w}^{*}}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

\newcommand{\mH}{m_{\mathcal H}}
\newcommand{\dvc}{{d_{\text{VC}}}}
\newcommand{\HH}[1]{\mathcal H_{\text{#1}}}
\newcommand{\Hbinary}{\HH_{\text{binary}}}
\newcommand{\Haxis}{\HH_{\text{axis}}}
\newcommand{\Hperceptron}{\HH_{\text{perceptron}}}


\newcommand{\ignore}[1]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\begin{center}
{
\Huge
%Chapter 1/2: The Learning Problem (III)
Chapter 2: Training vs Testing
}
\end{center}

\begin{center}
%\includegraphics[height=3in]{scooby}
%~~~~~~~~~~
%\includegraphics[height=3in]{ml}
\end{center}

\section*{Motivation}

The Finite Hypothesis Class Generalization (FHCG) Theorem from the previous lecture notes gives us the bound
\begin{equation}
    \Eout \le \Ein + O\bigg(\sqrt{\frac{\log M - \log\delta}{N}}\bigg)
    .
\end{equation}
This bound is not useful for infinite hypothesis classes because the resulting bound is trivial.

The goal of this chapter is to introduce a new way to measure the ``size'' of a hypothesis class called the \emph{VC dimension} ($\dvc$).
It turns out that many infinite hypothesis classes have finite VC dimension,
and the \emph{fundamental theorem of machine learning} states
\begin{equation}
    \Eout \le \Ein + O\bigg(\sqrt{\frac{\dvc - \log\delta}{N}}\bigg)
    .
\end{equation}
Proving the above bound is rather technical.
The textbook proves the slightly weaker bound
\begin{equation}
    \Eout \le \Ein + O\bigg(\sqrt{\frac{\dvc\log N - \log\delta}{N}}\bigg)
    .
\end{equation}
There are unfortunately a lot of technical details needed to understand the VC dimension.
This chapter is the most mathematically difficult part of this course.

\begin{note}
The textbook contains sections labeled ``safe skip'' that contain the full proof of the theorem above.
You are not responsible for the ``safely skippable'' portions of the chapter.
You are responsible for everything else.
\end{note}

\newpage
\section*{Section 2.1.1 Effective Number of Hypotheses}

\begin{defn}
    Let $\x_1,...,\x_N \in \mathcal X$.
    The \emph{dichotomies} generated by a hypothesis class $\mathcal H$ on these points are defined by
    \begin{equation}
        \mathcal H(\x_1, ..., \x_N) = \bigg\{ \big(h(\x_1), ..., h(\x_N)\big) : h \in \mathcal H \bigg\}
    \end{equation}
\end{defn}

\begin{example}
    Consider the dataset of 4 points defined by
    \begin{align*}
    \x_1 = (+1,+1) \\
    \x_2 = (-1,+1) \\
    \x_3 = (+1,-1) \\
    \x_4 = (-1,-1)
    \end{align*}
    and the dataset of 4 points defined by
    \begin{align*}
    \x'_1 = (+1,+1) \\
    \x'_2 = (-1,-1) \\
    \x'_3 = (+2,+2) \\
    \x'_4 = (-2,-2)
    \end{align*}
    What are the dichotomies generated by $\HH{axis}$ hypothesis class on these two datasets?
    Recall that
    \begin{equation*}
    \HH{axis} = \bigg\{ \x \mapsto \sign(x_i) : i \in [d] \bigg\}.
    \end{equation*}
\end{example}


%\begin{problem}
    %You should be able to calculate the dichotomies for any dataset / hypothesis class combination.
    %To practice this, compute the dichotomies for the other finite hypothesis classes from the previous notes.
%\end{problem}

\newpage
\begin{defn}
    The \emph{growth function} for a hypothesis class $\mathcal H$ is defined to be
    \begin{equation}
        m_{\mathcal H}(N) = \max_{\x_1,...,\x_N\in\mathcal X} \big| \mathcal H(\x_1, ..., \x_N) \big|.
    \end{equation}
\end{defn}
\begin{problem}
    [Example 2.1, page 43]
    Let $\mathcal H$ be the perceptron hypothesis class in 2 dimensions.
    What is $\mH(3)$ and $\mH(4)$?
\end{problem}
\vspace{2in}

\newpage
\begin{problem}
    Either prove the following statement, or find a counterexample:
    For all datasets and all hypothesis classes,
    %\begin{equation}
        $
        \mH(N) \le 2^N.
        $
    %\end{equation}
\end{problem}
%\vspace{2in}

\newpage
\begin{defn}
    We say that a hypothesis class $\mathcal H$ can \emph{shatter} a dataset $\x_1, ..., \x_N$ if any of the following equivalent statements are true:
    \begin{enumerate}
        \item $\mathcal H$ is capable of generating all possible dichotomies of $\x_1, ..., \x_N$.
        \item $\mathcal H(\x_1, ..., \x_N) = \{-1, +1\}^N$.
        \item $\left|\mathcal H(\x_1, ..., \x_N)\right| = 2^N$.
    \end{enumerate}
    %If $\mathcal H$ is capable of generating all possible dichotomies on $\x_1, ..., \x_N$ (i.e.\ $\mathcal H(\x_1, ..., \x_N) = \{-1, +1\}^N$), then we say that $\mathcal H$ can \emph{shatter} $\x_1, ..., \x_N$.
\end{defn}
\begin{defn}
    If no data set of size $k$ can be shattered by $\mathcal H$, then $k$ is said to be a \emph{break point} for $\mathcal H$.
\end{defn}

%\vspace{6in}
%\begin{problem}
    %Example 2.2 in the textbook (43-45) contains many more examples of computing the growth function.
    %You should work through and understand all of these examples.
%\end{problem}
\newpage

%\begin{fact}
    %The following two statements are equivalent:
    %\begin{enumerate}
        %\item $\mH(N) = 2^N$.
        %\item $\mathcal H$ shatters some dataset of size $N$.
    %\end{enumerate}
%\end{fact}

%\begin{problem}(Example 2.1)
    %If $\mathcal X=\R^2$ and $\mathcal H$ is the perceptron,
    %then what are $\mH(3)$ and $\mH(4)$?
%\end{problem}

%\newpage

\section*{Section 2.1.2: Bounding the Growth Function}

\begin{theorem}
    If $\mH(k) < 2^k$ for some value $k$, then
    \begin{equation}
        \mH(N)
        \le \sum_{i=0}^{k-1} {N \choose k}
        =
        O(N^{k-1})
        .
    \end{equation}
    This implies that, $\mH$ grows exponentially before its first breakpoint,
    and polynomially thereafter.
\end{theorem}

\newpage
\section*{Section 2.1.3 / 2.1.4: The VC Dimension}
%\section*{Section 2.1.4: The VC Generalization Bound}

\begin{defn}
    The Vapnik-Chervonenkis dimension (VC dimension) of a hypothesis set $\mathcal H$, denoted by $\dvc(\mathcal H)$ or simply $\dvc$, is the largest value of $N$ for which $\mH(N) = 2^N$.
    If $\mH(N) = 2^N$ for all $N$, then $\dvc = \infty$.
\end{defn}

%\begin{fact}
    %If $\mathcal H$ can shatter a set of size $N$, then $\dvc(\mathcal H) \ge N$.
%\end{fact}
%
%\begin{fact}
    %For any hypothesis class $\mathcal H$, the value $\dvc(\mathcal H) + 1$ is a break point for $\mathcal H$.
%\end{fact}

\begin{fact}[Equation 2.9/2.10, page 50]
    \label{fact:2.10}
    For all hypothesis classes $\mathcal H$, we have that
    \begin{equation}
        \label{eq:mHdvc}
        \mH(N) \le N^\dvc + 1
    \end{equation}
\end{fact}

%\vspace{3in}
\newpage
\begin{theorem}[VC generalization bound]
    For any tolerance $\delta>0$, we have that with probability at least $1-\delta,$
    \begin{equation}
        \Eout \le \Ein + \sqrt{\frac8N \log\frac{4\mH(2N)}{\delta}}.
    \end{equation}
    Substituting the bound from Fact \ref{fact:2.10} above,
    we get that
    \begin{align}
        \Eout 
        \le \Ein + \sqrt{\frac8N \log\frac{4(2N)^\dvc + 1}{\delta}} 
        = O\left(\sqrt{\frac{\dvc\log N - \log\delta}{N}}\right).
        %= \tilde O\left(\sqrt{\frac{\dvc - \log\delta}{N}}\right).
    \end{align}

\end{theorem}

\newpage
\begin{problem}
    What is the VC dimension of the perceptron hypothesis class?
\end{problem}

\newpage
\section*{Application (not in textbook)}
\begin{problem}
    You are a bank using the perceptron to learn a formula for whether or not to issue a loan.
    \begin{enumerate}
        \item
            You have successfully learned a model on the dataset $(\x_1,y_1), ..., (\x_N,y_N)$ where each $\x_i$ has $d$ features.
            Unfortunately, the training error of the model is too high.
            Management has allocated money to create a new dataset.
            Your choices are to either spend that money to add new features to the existing dataset,
            or to add more data points that all have the same features.
            According to VC theory, which action makes the most sense?

            \vspace{4in}
        \item
            You decided to augment the dataset so that it now has $2d$ features instead of only $d$ features.
            Now the generalization error is too high.
            According to VC theory, how many more data points will you need in order to achieve the same generalization error that you had before?
    \end{enumerate}
\end{problem}

%\begin{problem}
    %What is the VC dimension of the following hypothesis classes?
%
    %\begin{equation}
        %\HH{positive\_rays} = \bigg\{ x \mapsto \sign(x-a) : a \in \R \bigg\}
    %\end{equation}
%
%
    %\begin{equation}
        %\HH{positive\_intervals} = \bigg\{ x \mapsto 
    %\end{equation}
%\end{problem}

\end{document}



