\documentclass[10pt]{exam}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{stmaryrd}
\usepackage{etoolbox}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

\usepackage{listings}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{defn}{Definition}
\newtheorem{fact}{Fact}
\newtheorem{refr}{References}
\newtheorem{theorem}{Theorem}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\prob}{\mathbb P}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\Ein}{E_{\text{in}}}
\newcommand{\Eout}{E_{\text{out}}}
\newcommand{\Etest}{E_{\text{test}}}
\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\wstar}{{\w}^{*}}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

\newcommand{\mH}{m_{\mathcal H}}
\newcommand{\dvc}{{d_{\text{VC}}}}
\newcommand{\HH}[1]{\mathcal H_{\text{#1}}}
\newcommand{\Hbinary}{\HH_{\text{binary}}}
\newcommand{\Haxis}{\HH_{\text{axis}}}
\newcommand{\Hperceptron}{\HH_{\text{perceptron}}}


\newcommand{\ignore}[1]{}

\renewcommand{\solutiontitle}{}
%\AfterEndEnvironment{solutionorbox}{
    %\ifprintanswers\vspace{\stretch{1}}
    %\fi}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\printanswers

\begin{document}


\begin{center}
{
\Huge
Quiz: Chapter 1+2 definitions
}
\end{center}

\begin{center}
%\includegraphics[height=3in]{scooby}
%~~~~~~~~~~
%\includegraphics[height=3in]{ml}
\end{center}

\begin{defn}
    The \emph{in-sample error} is defined to be
    \begin{solutionorbox}[1in]
    \begin{equation*}
        \Ein(h) = \frac1N \sum_{i=1}^N \llbracket h(\x_i) \ne y_i \rrbracket
        .
    \end{equation*}
    \end{solutionorbox}
\end{defn}

\begin{defn}
    The \emph{out-of-sample error} is defined to be
    \begin{solutionorbox}[1in]
    \begin{equation*}
        \Eout(h) = \prob\big(h(\x) \ne y\big)
        .
    \end{equation*}
    \end{solutionorbox}
\end{defn}

\begin{defn}
    The true label function is defined to be
    \begin{solutionorbox}[1in]
    \begin{equation*}
        f = \argmin_{h \in \mathcal H^*} \Eout(h),
    \end{equation*}
    where $\mathcal H^*$ is the union of all hypothesis classes.
    \end{solutionorbox}
\end{defn}

\begin{defn}
    The \emph{generalization error} of a hypothesis $g$ is defined to be
    \begin{solutionorbox}[1in]
    $|\Ein(g) - \Eout(g)|$.
    \end{solutionorbox}
\end{defn}

\begin{defn}
    Let $\x_1,...,\x_N \in \mathcal X$.
    The \emph{dichotomies} generated by a hypothesis class $\mathcal H$ on these points are defined by
    \begin{solutionorbox}[1in]
    \begin{equation*}
        \mathcal H(\x_1, ..., \x_N) = \bigg\{ \big(h(\x_1), ..., h(\x_N)\big) : h \in \mathcal H \bigg\}
    \end{equation*}
    \end{solutionorbox}
\end{defn}

\newpage
\begin{defn}
    The \emph{growth function} for a hypothesis class $\mathcal H$ is defined to be
    \begin{solutionorbox}[1in]
    \begin{equation*}
        m_{\mathcal H}(N) = \max_{\x_1,...,\x_N\in\mathcal X} \big| \mathcal H(\x_1, ..., \x_N) \big|.
    \end{equation*}
    \end{solutionorbox}
\end{defn}

\begin{defn}
    We say that a hypothesis class $\mathcal H$ can \emph{shatter} a dataset $\x_1, ..., \x_N$ if any of the following equivalent statements are true:
    \begin{solutionorbox}[1in]
    \begin{enumerate}
        \item $\mathcal H$ is capable of generating all possible dichotomies of $\x_1, ..., \x_N$.
        \item $\mathcal H(\x_1, ..., \x_N) = \{-1, +1\}^N$.
        \item $\left|\mathcal H(\x_1, ..., \x_N)\right| = 2^N$.
    \end{enumerate}

        NOTE: You must list all 3 for full credit.
    \end{solutionorbox}
\end{defn}
\begin{defn}
The integer $k$ is said to be a \emph{break point} for hypothesis class $\mathcal H$ if
    \begin{solutionorbox}[1in]
    no data set of size $k$ can be shattered by $\mathcal H$.
    \end{solutionorbox}
\end{defn}


\ignore{
\begin{theorem}[Hoeffding Inequality]
    Let $a_1, ..., a_N$ be $N$ independent and identically distributed random variables satisfying $0 \le a_i \le 1$.
    Let $\nu = \tfrac1n\sum_{i=1}^N a_i$ be the empirical average and $\mu = \E \nu$ be the true mean of the underlying distribution.
    Then, for all $\epsilon > 0$,
    \begin{solutionorbox}[1in]
    \begin{equation*}
        \label{eq:hoef}
        %\prob\big(|S_n - \E[S_n]| \ge t\big)
        \prob\big(|\nu - \mu| \ge \epsilon\big)
        \le 
        2 \exp (-2\epsilon^2 N)
        .
    \end{equation*}
    \end{solutionorbox}
\end{theorem}

\begin{theorem}[Finite Hypothesis Class Generalization]
    %Equation (1.6) in the textbook states that for any hypothesis class $\mathcal H$ with $M$ entries
    Let $\mathcal H$ be a hypothesis class of size $M$,
    let $g$ be an arbitrary hypothesis in $\mathcal H$
    (in particular, $g$ is allowed to be the result of the TEA algorithm),
    and let $N$ be the size of the dataset.
    Then we have that for all $\epsilon>0$,
    \begin{equation*}
    \prob[|\Ein(g) - \Eout(g)| \ge \epsilon] \le 2M\exp(-2\epsilon^2 N).
    \end{equation*}
    This implies that with probability at least $1-\delta$,
    \begin{equation*}
        \Eout(g) \le \Ein(g) + \sqrt{\frac{1}{2N} \log\frac{2M}{\delta}}.
    \end{equation*}
\end{theorem}
}

\begin{defn}
    The Vapnik-Chervonenkis dimension (VC dimension) of a hypothesis class $\mathcal H$, denoted by $\dvc(\mathcal H)$ or simply $\dvc$, is 
    \begin{solutionorbox}[1in]
        the largest value of $N$ for which $\mH(N) = 2^N$.
        If $\mH(N) = 2^N$ for all $N$, then $\dvc = \infty$.
    \end{solutionorbox}
\end{defn}

\begin{theorem}[VC generalization bound]
    For any tolerance $\delta>0$, we have that with probability at least $1-\delta,$
    \begin{solutionorbox}[1in]
    \begin{equation*}
        \Eout 
         \le
         \Ein
         +
         O\left(\sqrt{\frac{\dvc\log N - \log\delta}{N}}\right).
    \end{equation*}
        NOTE: The more precise, non-asymptotic formulas would also be acceptable.
    \end{solutionorbox}

\end{theorem}

\end{document}



