\documentclass[10pt]{exam}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage{stmaryrd}

\usepackage{color}
\usepackage{colortbl}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{gray}{rgb}{0.7,0.7,0.7}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand*{\hl}[1]{\colorbox{yellow}{#1}}

\newcommand*{\answerLong}[2]{
    \ifprintanswers{\hl{#1}}
\else{#2}
\fi
}

\newcommand*{\answer}[1]{\answerLong{#1}{~}}

\newcommand*{\TrueFalse}[1]{%
\ifprintanswers
    \ifthenelse{\equal{#1}{T}}{%
        %\hl{\textbf{TRUE}}\hspace*{14pt}False
        \hl{\texttt{True}}\hspace*{20pt}\texttt{False}\hspace*{20pt}\texttt{Open}
    }{
        \ifthenelse{\equal{#1}{F}}{
        %True\hspace*{14pt}\hl{\textbf{FALSE}}
        \texttt{True}\hspace*{20pt}\hl{\texttt{False}}\hspace*{20pt}\texttt{Open}
        }
        {
            \texttt{True}\hspace*{20pt}{\texttt{False}}\hspace*{20pt}\hl{\texttt{Open}}
        }
    }
\else
    \texttt{True}\hspace*{20pt}\texttt{False}\hspace*{20pt}\texttt{Open}
\fi
} 
%% The following code is based on an answer by Gonzalo Medina
%% https://tex.stackexchange.com/a/13106/39194
\newlength\TFlengthA
\newlength\TFlengthB
\settowidth\TFlengthA{\hspace*{1.8in}}
\newcommand\TFQuestion[2]{%
    \setlength\TFlengthB{\linewidth}
    \addtolength\TFlengthB{-\TFlengthA}
    \noindent
    \parbox[t]{\TFlengthA}{\TrueFalse{#1}}\parbox[t]{\TFlengthB}{#2}
    \vspace{0.25in}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{refr}{References}
\newcommand{\E}{\mathbb E}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\nnz}{nnz}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\determinant}{det}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\I}{\mathbf I}
\newcommand{\Q}{\mathbf Q}
\newcommand{\p}{\mathbf P}
\newcommand{\pb}{\bar {\p}}
\newcommand{\pbb}{\bar {\pb}}
\newcommand{\pr}{\bm \pi}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\loss}{\ell}
\newcommand{\w}{\mathbf w}
\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\lone}[1]{{\lVert {#1} \rVert}_1}
\newcommand{\ltwo}[1]{{\lVert {#1} \rVert}_2}
\newcommand{\lp}[1]{{\lVert {#1} \rVert}_p}
\newcommand{\linf}[1]{{\lVert {#1} \rVert}_\infty}
\newcommand{\lF}[1]{{\lVert {#1} \rVert}_F}

\newcommand{\Ein}{E_{\text{in}}}
\newcommand{\Eout}{E_{\text{out}}}
\newcommand{\Etest}{E_{\text{test}}}
%\newcommand{\I}{\mathbf I}
%\newcommand{\Q}{\mathbf Q}
%\newcommand{\p}{\mathbf P}
%\newcommand{\pb}{\bar {\p}}
%\newcommand{\pbb}{\bar {\pb}}
%\newcommand{\pr}{\bm \pi}
\newcommand{\mH}{m_{\mathcal H}}
\newcommand{\dvc}{{d_{\text{VC}}}}
\newcommand{\HH}[1]{\mathcal H_{\text{#1}}}
\newcommand{\Hbinary}{\HH_{\text{binary}}}
\newcommand{\Haxis}{\HH_{\text{axis}}}
\newcommand{\Hperceptron}{\HH_{\text{perceptron}}}


\newcommand{\ignore}[1]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\printanswers

\begin{document}


\begin{center}
{
\Huge
    More Midterm 2 Practice Problems
}
\end{center}

%\vspace{0.5in}
%\noindent
%\textbf{Printed Name:}
%
%\noindent
%\rule{\textwidth}{0.1pt}
%\vspace{0.25in}

\begin{problem}
    For each statement below,
    circle \texttt{True} if the statement is known to be true,
    \texttt{False} if the statement is known to be false,
    and \texttt{Open} if the statement is not known to be either true or false.
    Ensure that you pay careful attention to the formal definitions of asymptotic notation in your responses.

\begin{enumerate}
    %\item\TFQuestion{T}{Let $g_1$ be the output of running the PLA on a training set, and $g_2$ the output of running the pocket algorithm.}
%
    %\item\TFQuestion{F}{After a finite number of iterations, the PLA is guaranteed to converge to a hypothesis 

    \item\TFQuestion{T}{For learning problems that are linearly separable, the fastest possible algorithm for minimizing the 0-1 loss runs in sub-exponential time in the number of feature dimensions $d$.}

    \item\TFQuestion{O}{For learning problems that are not linearly separable, the fastest possible algorithm for minimizing the 0-1 loss runs in sub-exponential time in the number of feature dimensions $d$.}

    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class.  Assume there exists some $h\in\mathcal H$ with $\Eout(h)=0$.  Then after a finite number of iterations, the PLA is guaranteed to output a hypothesis $g$ satisfying $\Ein(g) = 0$.}

    \item\TFQuestion{F}{Let $\mathcal H$ be the perceptron hypothesis class.  Assume there exists some $h\in\mathcal H$ with $\Ein(h)=0$.  Then after a finite number of iterations, the PLA is guaranteed to output a hypothesis $g$ satisfying $\Eout(g) = 0$.}

    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class.  Assume there exists some $h\in\mathcal H$ with $\Ein(h)=0$.  Then the PLA will terminate.}

    \item\TFQuestion{F}{Let $\mathcal H$ be the perceptron hypothesis class.  Let $g_1\in\mathcal H$ be the output of the PLA and $g_2\in\mathcal H$ be the output of the pocket algorithm, and assume that both algorithms successfully terminate in finite time.  The VC theory predicts that $g_2$ will have better generalization error than $g_1$ with high probability.  }

    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class and let $\mathcal H_\Phi$ be the perceptron hypothesis class with the 3rd degree polynomial feature map applied.  If you attempt to use the PLA to select a hypothesis $g\in\mathcal H$ and the algorithm terminates, then the PLA is also guaranteed to terminate if you use it to select a hypothesis from $\mathcal H_\Phi$.}

    \item\TFQuestion{T}{If your dataset is linearly separable, then the PLA is guaranteed to terminate.}

    \item\TFQuestion{T}{If your dataset is not linearly separable, then the pocket algorithm is guaranteed to terminate.}

    \item\TFQuestion{F}{Let $\mathcal H$ be the perceptron hypothesis class and let $g\in\mathcal H$ be the output of the pocket algorithm.  Then the Hoeffding inequality can be used to bound the generalization error of $g$ (i.e.\ the Hoeffding inequality can be used to bound $|\Ein(g)-\Eout(g)|$).}

    \item\TFQuestion{F}{Let $\mathcal H_1$ and $\mathcal H_2$ be two hypothesis classes with $\dvc(H_1) > \dvc(H_2)$.  Let $g_1\in H_1$  and $g_2\in H_2$.  Hoeffding's inequality predicts that $|\Etest(g_1) - \Eout(g_1)|$ is less than $|\Etest(g_2) - \Eout(g_2)|$.}

    \item\TFQuestion{F}{Let
        $$
    \HH{axis2} = \bigg\{ \x \mapsto \sigma\sign(x_i) : \sigma \in\{+1, -1\}, i \in [d] \bigg\},
        $$
        and
        $$
    \HH{L2-4} = \bigg\{ \x \mapsto \big\llbracket\ltwo{\x} \ge \alpha \big\rrbracket: \alpha \in \{1, 2, 3, 4\} \bigg\}.
        $$
        Let $g_{\text{axis2}} \in \HH{axis2}$ and $g_{\text{L2-4}}\in\HH{L2-4}$ be the outputs of the TEA algorithm on their respective hypothesis classes.
        Then for learning problems with large $d$, the following inequality is guaranteed to hold: $\Ein(g_{\text{axis2}}) \le \Ein(g_\text{L2-4})$.
        }

    \item\TFQuestion{F}{Let
        $$
    \HH{axis2} = \bigg\{ \x \mapsto \sigma\sign(x_i) : \sigma \in\{+1, -1\}, i \in [d] \bigg\},
        $$
        and
        $$
    \HH{L2-4} = \bigg\{ \x \mapsto \big\llbracket\ltwo{\x} \ge \alpha \big\rrbracket: \alpha \in \{1, 2, 3, 4\} \bigg\}.
        $$
        Let $g_{\text{axis2}} \in \HH{axis2}$ and $g_{\text{L2-4}}\in\HH{L2-4}$ be the outputs of the TEA algorithm on their respective hypothesis classes.
        Then for learning problems with large $d$, VC theory predicts that $g_{\text{L2-4}}$ will have better generalization accuracy than $g_{\text{axis2}}$.
        }

    \item\TFQuestion{T}{The VC dimension of finite hypothesis classes can never be $\infty$.}

    \item\TFQuestion{F}{The VC dimension of infinite hypothesis classes can never be $\infty$.}

    \item\TFQuestion{F}{Let $\mathcal H$ be a hypothesis class.  If there exists a hypothesis $h\in\mathcal H$ such that $\Eout(h) = 0$, then the VC dimension of $\mathcal H$ must be finite.}

    \item\TFQuestion{F}{Let $\mathcal H_1$ and $\mathcal H_2$ be two hypothesis classes satisfying $\mathcal H_1 \subset \mathcal H_2$.  Then the approximation error of $\mathcal H_1$ is guaranteed to be less than or equal to the approximation error of $\mathcal H_2$.}

    \item \TFQuestion{T}{For every learning problem, the approximation error is less than or equal to the in-sample error.}

    \item\TFQuestion{F}{You have a learning problem with a high approximation error.  VC theory predicts that increasing the number of data points will reduce the out-of-sample error.}

    \item\TFQuestion{T}{You have a learning problem with a high approximation error.  Applying the polynomial feature embedding with a high degree will decrease the approximation error.}

    \item\TFQuestion{F}{You have a learning problem with a high approximation error.  Applying the random feature embedding with a low output degree will decrease the approximation error.}

    \item\TFQuestion{T}{You have trained a logistic regression model that has high generalization error.  VC theory predicts that applying the PCA feature embedding with a low output dimension will reduce the generalization error.}

    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class and let $\mathcal H_\Phi$ be the perceptron hypothesis class with the decision stump feature map.  VC theory predicts that the generalization error of $\mathcal H_\Phi$ will be better than the generalization error of $\mathcal H$.}

    \item\TFQuestion{T}{Let $\mathcal H$ be the perceptron hypothesis class and let $\mathcal H_\Phi$ be the perceptron hypothesis class with the decision stump feature map.  Let $g\in\mathcal H$ and $g_\Phi \in \mathcal H_\Phi$ be the emperical risk minimizers trained on a very large dataset.  VC theory predicts that $\Eout(g) < \Eout(g_\Phi)$.}

    \item\TFQuestion{T}{The hinge loss is a surrogate loss function.}

    \item\TFQuestion{F}{The trimmed hinge loss is convex.}

\end{enumerate}
\end{problem}
\end{document}




