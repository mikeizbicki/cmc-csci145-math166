{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC Dimension, Halfspaces, and the Polynomial Kernel\n",
    "\n",
    "The perceptron hypothesis class is often called the hypothesis class of \"half spaces\" because it divides the space $R^d$ in half with a hyperplane.\n",
    "In this assignment, you will create various synthetic datasets, and explore how changing the hyperparameters of the halfspace hypothesis class with the polynomial kernel affects the statistical performance.\n",
    "The advantage of using synthetic data is that you can control various aspects of the data generation process to see how different sources of error result in different types of output.\n",
    "\n",
    "The main purpose of this assignment is to help give you intuition for all of the terms and formulas we've been defining in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell contains imports and global configurations.\n",
    "You shouldn't have to modify anything in this cell.\n",
    "'''\n",
    "\n",
    "# import standard python libraries libraries\n",
    "import math\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import time\n",
    "\n",
    "# disable warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# import data mining libraries\n",
    "import sklearn.linear_model\n",
    "\n",
    "# import and configure plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['lines.markersize'] = 1\n",
    "#plt.rcParams['figure.figsize'] = [9, 6]\n",
    "#plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# configurations for my code\n",
    "default_num_trials = 25\n",
    "max_d = 2**14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Exploring the VC Dimension of the polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel_embedding(x, p):\n",
    "    '''\n",
    "    Embeds the vector x into a higher dimensional space using the polynomial kernel of degree p.\n",
    "    The output dimension is equal to the VC-dimension of Theta(min{p**d,d**p}), where d is the dimension of x.\n",
    "    This is not an efficient implementation; it is designed for clarity.\n",
    "    \n",
    "    >>> polynomial_kernel_embedding(np.array([2]),1)\n",
    "    array([2])\n",
    "    >>> polynomial_kernel_embedding(np.array([2]),2)\n",
    "    array([2, 4])\n",
    "    >>> polynomial_kernel_embedding(np.array([2]),3)\n",
    "    array([2, 4, 8])\n",
    "    \n",
    "    >>> polynomial_kernel_embedding(np.array([2,3]),1)\n",
    "    array([2, 3])\n",
    "    >>> polynomial_kernel_embedding(np.array([2,3]),2)\n",
    "    array([2, 3, 4, 6, 9])\n",
    "    >>> polynomial_kernel_embedding(np.array([2,3]),3)\n",
    "    array([ 2,  3,  4,  6,  9,  8, 12, 18, 27])\n",
    "    \n",
    "    >>> polynomial_kernel_embedding(np.array(range(2)),10).shape\n",
    "    (65,)\n",
    "    >>> polynomial_kernel_embedding(np.array(range(3)),10).shape\n",
    "    (285,)\n",
    "    >>> polynomial_kernel_embedding(np.array(range(4)),10).shape\n",
    "    (1000,)\n",
    "    '''\n",
    "    assert type(x) is np.ndarray\n",
    "    assert len(x.shape) == 1\n",
    "    assert type(p) is int\n",
    "    assert p > 0\n",
    "    \n",
    "    d = x.shape[0]\n",
    "    terms_per_degree = [ [ [i] for i in range(d) ] ]\n",
    "    for i in range(1,p):\n",
    "        deg_i_terms = []\n",
    "        deg_i_minus1_terms = terms_per_degree[-1]\n",
    "        for k in range(len(deg_i_minus1_terms)):\n",
    "            for j in range(deg_i_minus1_terms[k][-1],d):\n",
    "                deg_i_terms.append(deg_i_minus1_terms[k]+[j])\n",
    "        terms_per_degree.append(deg_i_terms)\n",
    "    \n",
    "    terms = [ inner for outer in terms_per_degree for inner in outer ]\n",
    "    \n",
    "    values = []\n",
    "    for term in terms:\n",
    "        value = 1\n",
    "        for i in term:\n",
    "            value *= x[i]\n",
    "        values.append(value)\n",
    "    return np.array(values)\n",
    "    \n",
    "    embeddings = [x]\n",
    "    for i in range(1,p):\n",
    "        deg_i_terms = []\n",
    "        deg_i_minus1_terms = embeddings[-1]\n",
    "        for j in range(x.shape[0]):\n",
    "            for k in range(deg_i_minus1_terms.shape[0]):\n",
    "                deg_i_terms.append(x[j]*deg_i_minus1_terms[k])\n",
    "        embeddings.append(np.array(deg_i_terms))\n",
    "    return np.concatenate(embeddings)\n",
    "\n",
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots the VC-dimension of a halfspace with the polynomial kernel\n",
    "as a function of the degree p with a fixed dimension d\n",
    "'''\n",
    "ps = range(1,50)\n",
    "d = 2\n",
    "vcdims = [ polynomial_kernel_embedding(np.ones([d]),p).shape for p in ps ]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.ylabel('VC-dimension')\n",
    "plt.xlabel('polynomial degree = p')\n",
    "plt.plot(ps,vcdims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots the VC-dimension of a halfspace with the polynomial kernel\n",
    "as a function of the dimension d with a fixed p\n",
    "'''\n",
    "ds = range(1,50)\n",
    "p = 2\n",
    "vcdims = [ polynomial_kernel_embedding(np.ones([d]),p).shape for d in ds ]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.ylabel('VC-dimension')\n",
    "plt.xlabel('input dimension = d')\n",
    "plt.plot(ps,vcdims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots the VC-dimension of a halfspace with the polynomial kernel\n",
    "as the dimension and degree both increase\n",
    "'''\n",
    "dps = range(1,10)\n",
    "vcdims = [ polynomial_kernel_embedding(np.ones([dp]),dp).shape for dp in dps ]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.ylabel('VC-dimension')\n",
    "plt.xlabel('polynomial degree = p = input dimension = d')\n",
    "plt.plot(dps,vcdims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "*The plots above visualize the VC-dimension of halfspaces with the polynomical kernel.\n",
    "In particular, they show regimes where the VC-dimension grows polynomially and regimes where it grows exponentially.*\n",
    "\n",
    "*What is the formula for the VC-dimension of halfspaces with the polynomial kernel?*\n",
    "\n",
    "Type your answers below each question in non-italic text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "*In most practical problems, the dimension $d$ of the input dataspace $\\mathcal X$ is large ( think $d > 10^{6}$).\n",
    "Large degree polynomial kernels ($p>3$) are not typically used in this situation.\n",
    "Explain the downsides of using a large degree polynomial kernel in terms of sample complexity using VC theory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: generating synthetic data with different properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell contains a number of functions for generating and visualizing datasets.\n",
    "'''\n",
    "\n",
    "def generate_dataset(m, d, f, sigma, seed=0):\n",
    "    '''\n",
    "    Returns a dataset with m data points of dimension d generated by:\n",
    "    \n",
    "        X ~ Uniform(-1, 1)\n",
    "        Y = f(X) + epsilon, where epsilon ~ Normal(0,sigma)\n",
    "        \n",
    "    When sigma=0, there is no randomness in Y, and so the bayes error will be 0\n",
    "    When sigma>0, there is randomness in Y, and so the bayes error > 0\n",
    "    '''\n",
    "    # set numpy's seed for reproducable results\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # ensure reasonable input parameters\n",
    "    assert type(m) is int\n",
    "    assert m>0\n",
    "    \n",
    "    # a helper function for generating labels\n",
    "    def sign(a):\n",
    "        if a>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    # generate the features\n",
    "    X = np.random.uniform(low=-1.0,high=1.0,size=[m,d])\n",
    "\n",
    "    # generate the labels\n",
    "    Ys = []\n",
    "    for i in range(m):\n",
    "        epsilon = np.random.randn()*sigma\n",
    "        yi = sign(f(X[i]) + epsilon)\n",
    "        Ys.append(yi)\n",
    "    Y = np.array(Ys)\n",
    "    \n",
    "    # return the dataset\n",
    "    return (X,Y)\n",
    "\n",
    "\n",
    "def f_polynomial(p, seed=0):\n",
    "    '''\n",
    "    Implements a polynomial embedding of degree $p$ to perform the data labeling.\n",
    "    If the halfspace hypothesis class with the polynomial kernel of degree > p is used for learning,\n",
    "    then the hypothesis class will be realizable.\n",
    "    The input dimension $d$ must match the dimension of the dataspace $\\mathcal X$\n",
    "    '''\n",
    "    phi = lambda x: polynomial_kernel_embedding(x, p)\n",
    "    embedding_dim = phi(np.ones([d])).shape[0]\n",
    "    np.random.seed(seed)\n",
    "    #w = (np.array(range(embedding_dim))/embedding_dim)**0.01\n",
    "    #w = np.ones([embedding_dim])/embedding_dim\n",
    "    w = np.random.randn(max_d)/math.sqrt(embedding_dim)\n",
    "    def f(x):\n",
    "        phi_x = phi(x)\n",
    "        w2 = w[:phi_x.shape[0]]\n",
    "        return phi_x.transpose() @ w2\n",
    "    return f\n",
    "    \n",
    "    \n",
    "def f_checkers():\n",
    "    '''\n",
    "    This is another embedding function that creates a \"checkers\" pattern in the data.\n",
    "    This embedding is not realizable for halfspaces with the polynomial kernel. \n",
    "    '''\n",
    "    p = 4\n",
    "    def f(x):\n",
    "        return math.sin(x[0]*p)*math.sin(x[1]*p)\n",
    "    return f\n",
    "\n",
    "    \n",
    "def f_circles():\n",
    "    '''\n",
    "    This is another embedding function that creates a pattern of concentric circles in the data.\n",
    "    This embedding is not realizable for halfspaces with the polynomial kernel. \n",
    "    '''\n",
    "    p = 4\n",
    "    def f(x):\n",
    "        return math.cos((x[0]*x[0]+x[1]*x[1])*p)\n",
    "    return f\n",
    "    \n",
    "\n",
    "def plot_dataset(S):\n",
    "    '''\n",
    "    Plots the first two dimensions of the input dataset.\n",
    "    '''\n",
    "    X,Y = S\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0],X[:,1],c=Y)\n",
    "    #ax.scatter(X_pos[:,0], X_pos[:,1])\n",
    "    #ax.scatter(X_neg[:,0], X_neg[:,1])\n",
    "    ax.set_xlim(-1.0,1.0)\n",
    "    ax.set_ylim(-1.0,1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots a dataset that has no randomness in the generation of class labels (sigma=0).\n",
    "Using our notation from the textbook, this means that for each i, y_i = f(x_i).\n",
    "When p=1 in the f_polynomial function, the data has a linear decision boundary.\n",
    "'''\n",
    "\n",
    "# these \"hyperparameters\" control the properties of the dataset\n",
    "d = 2\n",
    "sigma = 0\n",
    "f = f_polynomial(p=1,seed=2)\n",
    "\n",
    "# generate and plot dataset\n",
    "S = generate_dataset(m=2**16,d=d,f=f,sigma=sigma)\n",
    "plot_dataset(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "*Regenerate the dataset above several times with degree $1 \\le p \\le 10$ to explore what decision boundaries for higher degree polynomials look like.\n",
    "You should also change the `seed` value to generate different datasets with the same degree.*\n",
    "\n",
    "*Write a 1-2 sentence summary of your findings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots a dataset with randomness (sigma>0).\n",
    "The sigma variable controls the amount of randomness applied to the model's label. \n",
    "'''\n",
    "\n",
    "# these \"hyperparameters\" control the properties of the dataset\n",
    "d = 2\n",
    "sigma = 0.2\n",
    "f = f_polynomial(2)\n",
    "\n",
    "# generate and plot dataset\n",
    "S = generate_dataset(m=2**16,d=d,f=f,sigma=sigma)\n",
    "plot_dataset(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "*Experiment with different combinations of sigma (and different p values) to see how the level of randomness effects the data.*\n",
    "\n",
    "*Write a 1-2 sentence summary of your findings.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell plots a dataset in the PAC model in higher dimensions.\n",
    "Notice that even though there is no randomness in the labels,\n",
    "there appears to be randomness when we only visualize the data in 2 dimensions.\n",
    "'''\n",
    "\n",
    "# these \"hyperparameters\" control the properties of the dataset\n",
    "d = 4\n",
    "sigma = 0\n",
    "f = f_polynomial(d,2)\n",
    "\n",
    "# generate and plot dataset\n",
    "S = generate_dataset(m=2**16,d=d,f=f,sigma=sigma)\n",
    "plot_dataset(S)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "*Experiment with different values of `d` in the dataset plotted above.\n",
    "Also experiment with different values of `p` and `sigma`.\n",
    "Notice that in high dimensions it is very difficult to tell if the data is linearly separable, or if the data contains any randomness.\n",
    "Write 1-2 sentences explaining why this is the case.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: plotting the error as a function of the sample size $m$\n",
    "\n",
    "NOTE: The textbook uses the variable $N$ to measure the total number of data points, but this notebook uses the variable $m$ to denote the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_err_vs_m(d, sigma, f, p, num_trials=default_num_trials):\n",
    "    '''\n",
    "    Plots the sample/true/generalization error for the halfspace with polynomial kernel model\n",
    "    as a function of the number of sample training points.\n",
    "    '''\n",
    "\n",
    "    max_m_exp = 16\n",
    "    m_buffer = 3\n",
    "    \n",
    "    # S_test is our test set used for measuring the model's performance;\n",
    "    # It has a very large size to ensure that the empirical risk on S_test is very close to the true risk\n",
    "    print('generating dataset... ',end='')\n",
    "    S_test = generate_dataset(m=2**(max_m_exp+m_buffer),d=d,f=f,sigma=sigma)\n",
    "    print('done')\n",
    "    plot_dataset(S_test)\n",
    "\n",
    "    # these lists store the computer training and test errors\n",
    "    test_errs = []\n",
    "    train_errs = []\n",
    "    \n",
    "    # This is the list of all sample sizes we will train models on and generate train/test errors;\n",
    "    # by adjusting the range(), you can adjust the x-axis in the plots below.\n",
    "    ms = [ 2**i for i in range(0,max_m_exp) ]\n",
    "\n",
    "    for m in ms:\n",
    "    #for p in [1,2,3,4,5,6,7]:\n",
    "\n",
    "        # In order to \"smooth\" the plots, we will repeat each experiment multiple times\n",
    "        # as deterimed by the num_trials parameter.\n",
    "        # These lists store the raw results from each trial.\n",
    "        trials_test_accs = []\n",
    "        trials_train_accs = []\n",
    "        \n",
    "        # loop over each trial\n",
    "        seed_base = 10\n",
    "        time_start = time.time()\n",
    "        for seed in range(seed_base,seed_base+num_trials):\n",
    "            \n",
    "            # generate a training set of size m\n",
    "            # from the same distribution as our test set;\n",
    "            # notice that we must explicitly set a unique seed for each trial so that\n",
    "            # each iteration is actually running on a different training set\n",
    "            S_train = generate_dataset(m=m,d=d,f=f,sigma=sigma,seed=seed)\n",
    "\n",
    "            try:   \n",
    "                # train a linear model;\n",
    "                # notice that the training currently uses the LogisticRegression model;\n",
    "                # all of the results will be essentially the same using the other linear models as well\n",
    "                X, Y = S_train\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                #h_S = sklearn.linear_model.LogisticRegression(solver='liblinear',C=1e10)\n",
    "                #h_S = sklearn.linear_model.Perceptron()\n",
    "                h_S = sklearn.linear_model.SGDClassifier()\n",
    "                #h_S = sklearn.linear_model.PassiveAggressiveClassifier()\n",
    "                #h_S = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "                #h_S = sklearn.svm.LinearSVC()\n",
    "                h_S.fit(X, Y)\n",
    "\n",
    "                # calculate the training accuracy\n",
    "                train_acc = h_S.score(X,Y)\n",
    "\n",
    "                # calculate the test accuracy\n",
    "                X, Y = S_test\n",
    "                X = X[:min(2048,m_buffer*m)]\n",
    "                Y = Y[:min(2048,m_buffer*m)]\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                test_acc = h_S.score(X, Y)        \n",
    "\n",
    "            # ValueError raised when there's not enough data to perform classification;\n",
    "            # in this case, we get perfect training accuracy, but perfectly wrong test accuracy\n",
    "            except ValueError:\n",
    "                train_acc = 1\n",
    "                test_acc = 0\n",
    "                \n",
    "            trials_test_accs.append(test_acc)\n",
    "            trials_train_accs.append(train_acc)\n",
    "        time_end = time.time()\n",
    "        \n",
    "        # compute the average of our trials\n",
    "        train_acc = np.mean(trials_train_accs)\n",
    "        test_acc = np.mean(trials_test_accs)\n",
    "            \n",
    "        # print a debugging statement for each iteration\n",
    "        print('m=%8d,  train_acc=%0.4f,  test_acc=%0.4f,  time_diff=%dsec'%(\n",
    "            m,\n",
    "            train_acc,\n",
    "            test_acc,\n",
    "            time_end-time_start\n",
    "        ))\n",
    "\n",
    "        # convert the accuracies into errors and store them\n",
    "        train_errs.append(1-train_acc)\n",
    "        test_errs.append(1-test_acc)\n",
    "    \n",
    "    # plot the errors\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(14,5))\n",
    "    ax1.set_xscale('log')\n",
    "    #ax1.set_yscale('log')\n",
    "    ax1.set_ylim([0.0,1.0])\n",
    "    ax1.set(\n",
    "        xlabel='number of samples = m', \n",
    "        ylabel='train error = E_in(g)',\n",
    "    )\n",
    "    ax1.plot(ms,train_errs)\n",
    "    \n",
    "    ax2.set_xscale('log')\n",
    "    #ax2.set_yscale('log')\n",
    "    ax2.set_ylim([0.0,1.0])\n",
    "    ax2.set(\n",
    "        xlabel='number of samples = m', \n",
    "        ylabel='test error ≈ E_out(g)',\n",
    "    )\n",
    "    ax2.plot(ms,test_errs)\n",
    "    \n",
    "    ax3.set_xscale('log')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set(\n",
    "        xlabel='number of samples = m', \n",
    "        ylabel='generalization error = |E_in(g) - E_out(g)|',\n",
    "\n",
    "    )\n",
    "    ax3.plot(ms,np.abs(np.array(test_errs)-np.array(train_errs)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0,\n",
    "    f=f_polynomial(p=2),\n",
    "    p=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0,\n",
    "    f=f_polynomial(p=2),\n",
    "    p=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0.2,\n",
    "    f=f_polynomial(p=2),\n",
    "    p=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "We briefly introduce some common definitions that are not found in the textbook and we have not directly discussed in class.\n",
    "These definitions will help us more succinctly state the following questions.\n",
    "Another purpose of these definitions is to give you practice working with new definitions that you haven't seen explained before.\n",
    "Data mining literature very frequently introduces new \n",
    "\n",
    "\n",
    "We call the \"approximation error\" of a hypothesis class to be the true error of the best possible model in the hypothesis class.\n",
    "That is, the approximation error is\n",
    "$$\n",
    "E_{approx}(\\mathcal{H}) = \\text{argmin}_{h\\in \\mathcal H} E_{out}(h).\n",
    "$$\n",
    "Notice that the approximation error depends both on the hypothesis class chosen for learning, but also on the data distribution (since $E_out$ depends on the data distribution).\n",
    "It does not, however, depend on the training data.\n",
    "\n",
    "We call a hypothesis class \"realizable\" when the approximation error is 0.\n",
    "That is, there exists some $h \\in \\mathcal H$ with $E_{out}(h) = 0$.\n",
    "Notice that this definition is independent of the training data and the training algorithm.\n",
    "\n",
    "### Question 3.1\n",
    "\n",
    "*The three cells above plot the accuracy of the polynomial kernel in three situations:\n",
    "when the model is not realizable because the degree of the polynomial is too small,\n",
    "when the model is realizable,\n",
    "and when the model is not realizable because of randomness in the labeling process.*\n",
    "\n",
    "*Write 1-2 sentences about the differences you observe in each of these cases and how they relate to VC theory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0,\n",
    "    f=f_checkers(),\n",
    "    p=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "\n",
    "*The dataset above is generated from the checkboard embedding instead of the polynomial embedding.\n",
    "This implies that the polynomial embedding model is non-realizable for this problem.\n",
    "Therefore, even when there is no randomness in the labeling process,\n",
    "the model still cannot achieve 0 approximation error (i.e. $E_{out}(h)>0$ for all $h \\in E_{out}(h)$.*\n",
    "\n",
    "*Increasing the polynomial degree $p$ reduces the approximation error.\n",
    "Try each value of $p$ starting at 1 until you reach an approximation error<1%.\n",
    "What is that value of $p$?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_m(\n",
    "    d=2,\n",
    "    sigma=0,\n",
    "    f=f_circles(),\n",
    "    p=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "\n",
    "*The dataset above is also not realizable, but in a different way than in question 3.2.\n",
    "Once again, your task is to find the exact $p$ value that will achieve an approximation error<0.01%.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: plotting the error as a function of the sample size $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_err_vs_d(m, sigma, f, p, max_dim_exponent, num_trials=default_num_trials):\n",
    "    '''\n",
    "    Plots the sample/true/generalization error for the halfspace with polynomial kernel model\n",
    "    as a function of the number of sample training points.\n",
    "    '''\n",
    "    m_buffer = 3\n",
    "    assert(2**max_dim_exponent<max_d)\n",
    "\n",
    "    # these lists store the computer training and test errors\n",
    "    test_errs = []\n",
    "    train_errs = []\n",
    "    \n",
    "    # This is the list of all sample sizes we will train models on and generate train/test errors;\n",
    "    # by adjusting the range(), you can adjust the x-axis in the plots below.\n",
    "    ds = [ 2**i for i in range(0,max_dim_exponent) ]\n",
    "\n",
    "    for d in ds:\n",
    "\n",
    "        # In order to \"smooth\" the plots, we will repeat each experiment multiple times\n",
    "        # as deterimed by the num_trials parameter.\n",
    "        # These lists store the raw results from each trial.\n",
    "        trials_test_accs = []\n",
    "        trials_train_accs = []\n",
    "        \n",
    "        \n",
    "        # loop over each trial\n",
    "        seed_base = 10\n",
    "        time_start = time.time()\n",
    "        for seed in range(seed_base,seed_base+num_trials):\n",
    "            \n",
    "            # generate a training set of size m\n",
    "            # from the same distribution as our test set;\n",
    "            # notice that we must explicitly set a unique seed for each trial so that\n",
    "            # each iteration is actually running on a different training set\n",
    "            S_train = generate_dataset(m=m,d=d,f=f,sigma=sigma,seed=seed)\n",
    "            S_test = generate_dataset(m=m,d=d,f=f,sigma=sigma,seed=seed-1)\n",
    "\n",
    "            try:   \n",
    "                # train a linear model;\n",
    "                # notice that the training currently uses the LogisticRegression model;\n",
    "                # all of the results will be essentially the same using the other linear models as well\n",
    "                # since they all use the same hypothesis class\n",
    "                X, Y = S_train\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                h_S = sklearn.linear_model.LogisticRegression(solver='liblinear',C=1e1)\n",
    "                #h_S = sklearn.linear_model.Perceptron()\n",
    "                #h_S = sklearn.linear_model.SGDClassifier()\n",
    "                #h_S = sklearn.linear_model.PassiveAggressiveClassifier()\n",
    "                #h_S = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "                #h_S = sklearn.svm.LinearSVC()\n",
    "                h_S.fit(X, Y)\n",
    "\n",
    "                # calculate the training accuracy\n",
    "                train_acc = h_S.score(X,Y)\n",
    "\n",
    "                # calculate the test accuracy\n",
    "                X, Y = S_test\n",
    "                X = np.apply_along_axis(lambda x: polynomial_kernel_embedding(x,p),1,X)\n",
    "                test_acc = h_S.score(X, Y)        \n",
    "\n",
    "            # ValueError raised when there's not enough data to perform classification;\n",
    "            # in this case, we get perfect training accuracy, but perfectly wrong test accuracy\n",
    "            except ValueError:\n",
    "                train_acc = 1\n",
    "                test_acc = 0\n",
    "                \n",
    "            trials_test_accs.append(test_acc)\n",
    "            trials_train_accs.append(train_acc)\n",
    "        time_end = time.time()\n",
    "        \n",
    "        # compute the average of our trials\n",
    "        train_acc = np.mean(trials_train_accs)\n",
    "        test_acc = np.mean(trials_test_accs)\n",
    "            \n",
    "        # print a debugging statement for each iteration\n",
    "        print('d=%8d,  train_acc=%0.4f,  test_acc=%0.4f,  time_diff=%dsec'%(\n",
    "            d,\n",
    "            train_acc,\n",
    "            test_acc,\n",
    "            time_end-time_start\n",
    "        ))\n",
    "\n",
    "        # convert the accuracies into errors and store them\n",
    "        train_errs.append(1-train_acc)\n",
    "        test_errs.append(1-test_acc)\n",
    "    \n",
    "    # plot the errors\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(14,5))\n",
    "    ax1.set_xscale('log',basex=2)\n",
    "    #ax1.set_yscale('log')\n",
    "    ax1.set(\n",
    "        xlabel='number of dimensions = d', \n",
    "        ylabel='train error = E_in(g)',\n",
    "    )\n",
    "    ax1.plot(ds,train_errs)\n",
    "    \n",
    "    ax2.set_xscale('log',basex=2)\n",
    "    #ax2.set_yscale('log')\n",
    "    ax2.set(\n",
    "        xlabel='number of dimensions = d', \n",
    "        ylabel='test error ≈ E_out(g)',\n",
    "    )\n",
    "    ax2.plot(ds,test_errs)\n",
    "    \n",
    "    ax3.set_xscale('log',basex=2)\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set(\n",
    "        xlabel='number of dimensions = d', \n",
    "        ylabel='generalization error = |E_in(g) - E_out(g)|',\n",
    "\n",
    "    )\n",
    "    ax3.plot(ds,np.abs(np.array(test_errs)-np.array(train_errs)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_d(\n",
    "    m = 512,\n",
    "    sigma = 0.1,\n",
    "    f = f_polynomial(p=1),\n",
    "    p = 1,\n",
    "    max_dim_exponent=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "\n",
    "*How does increasing the dimension $d$ of the input space affect the errors of the model?\n",
    "How does VC theory predict this behavior?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_err_vs_d(\n",
    "    m = 512,\n",
    "    sigma = 0.1,\n",
    "    f = f_polynomial(p=2),\n",
    "    p = 1,\n",
    "    max_dim_exponent=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2\n",
    "\n",
    "*The plot above is generated using almost the same formula as the plot for 4.1.\n",
    "The exception is that the $f$ function is more complicated (polynomial degree 2 instead of degree 1).\n",
    "What effect does this have on the the errors?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: plotting the error as a function of the polynomial degree\n",
    "\n",
    "### Question 5.1\n",
    "\n",
    "The goal of this question is to demonstrate that large degree polynomial embeddings overfit.\n",
    "This will require writing a new function `calculate_err_vs_p` that is based off of `calculate_err_vs_m` and `calculate_err_vs_d`.\n",
    "Plot the train, test, and generalization errors as a function of `p`.\n",
    "You should find that \"small\" and \"large\" values both generate large test errors,\n",
    "and \"medium\" values of `p` generate small test errors.\n",
    "\n",
    "I recommend using values of `m = 32`, `d = 8`, `sigma = 0.0`, and `f = f_polynomial(p=2)`.\n",
    "Adjust the value of the `p` variable starting at `1` and going large enough to see the pattern of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
